{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"},"colab":{"name":"0_5_autogradqs_tutorial_jp.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"eaX0zo5HYQLp"},"source":["「PyTorch入門  5. 自動微分」\n","===============================================================\n","【原題】AUTOMATIC DIFFERENTIATION WITH TORCH.AUTOGRAD\n","\n","【原著】\n","[Suraj Subramanian](https://github.com/suraj813)、[Seth Juarez](https://github.com/sethjuarez/) 、[Cassie Breviu](https://github.com/cassieview/) 、[Dmitry Soshnikov](https://soshnikov.com/)、[Ari Bornstein](https://github.com/aribornstein/) \n","\n","\n","【元URL】https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\n","\n","【翻訳】電通国際情報サービスISID AIトランスフォーメーションセンター　小川 雄太郎\n","\n","【日付】2021年03月18日\n","\n","【チュトーリアル概要】\n","\n","本チュートリアルでは、PyTorchで計算履歴を保存し、自動で微分操作を実現するAUTOGRADについて解説を行います。\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"beL67QpFYGRe"},"source":["\n","\n","Automatic Differentiation with ``torch.autograd``\n","=======================================\n","\n","ニューラルネットワークを訓練する際、その学習アルゴリズムとして、基本的には**バックプロパゲーション（back propagation）**が使用されます。\n","\n","バックプロパゲーションでは、モデルの重みなどの各パラメータは、損失関数に対するその変数の微分値（勾配）に応じて調整されます。\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Mer5Ga0bZDWf"},"source":["これらの勾配の値を計算するために、PyTorchには``torch.autograd`` という微分エンジンが組み込まれています。\n","\n","autogradはPyTorchの計算グラフに対する勾配の自動計算を支援します。\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nRFkPEn6ZXc9"},"source":["シンプルな1レイヤーのネットワークを想定しましょう。\n","\n","入力を``x``、パラメータを``w`` と ``b``、そして適切な損失関数を決めます。\n","\n","<br>\n","\n","PyTorchでは例えば以下のように実装します。\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"s3gJXefcYGRY","executionInfo":{"status":"ok","timestamp":1616112637213,"user_tz":-540,"elapsed":681,"user":{"displayName":"小川雄太郎","photoUrl":"","userId":"06190430902934159529"}}},"source":["%matplotlib inline"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"CNsj4b7HYGRg","executionInfo":{"status":"ok","timestamp":1616112640217,"user_tz":-540,"elapsed":3680,"user":{"displayName":"小川雄太郎","photoUrl":"","userId":"06190430902934159529"}}},"source":["import torch\n","\n","x = torch.ones(5)  # input tensor\n","y = torch.zeros(3)  # expected output\n","w = torch.randn(5, 3, requires_grad=True)\n","b = torch.randn(3, requires_grad=True)\n","z = torch.matmul(x, w)+b\n","loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L0GkOZYg0hwN"},"source":["---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cwElQF4fYGRg"},"source":["テンソル、関数、計算グラフの関係\n","------------------------------------------\n","\n","上記のコードは以下の**計算グラフ(computational graph)**を示しています。\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sh8g91IfaRBC"},"source":["<img src=\"https://pytorch.org/tutorials/_images/comp-graph.png\" width=50% alt=\"海の写真\" title=\"空と海\">"]},{"cell_type":"markdown","metadata":{"id":"b_Nh8JgEafTJ"},"source":["上記のネットワークでは、``w``と``b``が最適したいパラメータです。\n","\n","そのため、これらの変数に対する損失関数の微分値を計算する必要があります。\n","\n","これらのパラメータで微分を可能にするために、``requires_grad``属性をこれらのテンソルに追記します。\n"]},{"cell_type":"markdown","metadata":{"id":"VvYX1_pEYGRh"},"source":["【注意】\n","\n","``requires_grad``はテンソルを定義する際、もしくはその後に、``x.requires_grad_(True)``を実行するなどして指定できます。\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PIhzC5DQYGRh"},"source":["計算グラフを構築する際に、テンソルに適用する関数は実際には ``Function``クラスのオブジェクトです。\n","\n","これらオブジェクトでは、順伝搬時に入力をどのように処理するのか定義されています。\n","\n","加えて、バックプロパゲーション時に勾配をどのように計算するのかも把握しています。\n","\n","<br>\n","\n","そして勾配は、テンソルの ``grad_fn`` プロパティに格納されます。\n","\n"," ``Function``のさらなる詳細については、[こちら](https://pytorch.org/docs/stable/autograd.html#function)を参照ください。\n"]},{"cell_type":"code","metadata":{"id":"7-becalIYGRh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616112824793,"user_tz":-540,"elapsed":440,"user":{"displayName":"小川雄太郎","photoUrl":"","userId":"06190430902934159529"}},"outputId":"cabb767a-118c-4685-86da-ba750e7b7d98"},"source":["print('Gradient function for z =',z.grad_fn)\n","print('Gradient function for loss =', loss.grad_fn)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Gradient function for z = <AddBackward0 object at 0x7f7a86429b10>\n","Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward object at 0x7f7a86429b50>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BxLrmXP91QjT"},"source":["---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nrcp4v5MYGRh"},"source":["勾配の計算\n","-------------------\n","ニューラルネットワークの各パラメータを最適化するために、入力``x``と出力``y``が与えられたもとで、損失関数の各変数の偏微分値、\n","\n","すなわち\n","\n","$\\frac{\\partial loss}{\\partial w}$ 、$\\frac{\\partial loss}{\\partial b}$ \n","\n","を求める必要があります。\n","\n","\n","これらの偏微分値を求めるために``loss.backward()``を実行し、``w.grad``と``b.grad``の値を導出します。\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"AjVqNevVYGRi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616112889607,"user_tz":-540,"elapsed":487,"user":{"displayName":"小川雄太郎","photoUrl":"","userId":"06190430902934159529"}},"outputId":"22fb639d-be55-4352-b062-1222fad845f9"},"source":["loss.backward()\n","print(w.grad)\n","print(b.grad)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["tensor([[0.1212, 0.1953, 0.3046],\n","        [0.1212, 0.1953, 0.3046],\n","        [0.1212, 0.1953, 0.3046],\n","        [0.1212, 0.1953, 0.3046],\n","        [0.1212, 0.1953, 0.3046]])\n","tensor([0.1212, 0.1953, 0.3046])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UWBXGCbPYGRi"},"source":["【注意】\n","\n","``grad``は計算グラフのleaf node（かつ、``requires_grad``が``True``の変数）のみで求めることができます。\n","\n","全ての変数で勾配が計算できるわけではない点にはご注意ください。\n","\n","<br>\n","\n","また、勾配の計算は各計算グラフに対して、``backward``を実行し、1度だけ計算できます（1度しか計算できないのは、パフォーマンスの観点からの仕様です）。\n","\n","仮に、同じ計算グラフに対して複数回``backward``を実行したい場合には、``retain_graph=True``を``backward``時に引数として渡す必要があります。\n"]},{"cell_type":"markdown","metadata":{"id":"ooSqXVJF1xAU"},"source":["---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3Zdr0N-dYGRi"},"source":["勾配計算をしない方法\n","---------------------------\n","\n","デフォルトでは全てのテンソルは``requires_grad=True``であり、計算履歴が保持され、勾配計算可能な状態です。\n","\n","ですが、勾配計算が不要なケースも存在します。\n","\n","例えば訓練済みモデルで推論するケースなどです。\n","\n","すなわち、ネットワークの順伝搬関数のみを使用する場合となります。\n","\n","実装コードで勾配計算を不要にするには、``torch.no_grad()``のブロックにそれらのコードを記載するようにします。\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"pNqQsSOEYGRj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616113028228,"user_tz":-540,"elapsed":470,"user":{"displayName":"小川雄太郎","photoUrl":"","userId":"06190430902934159529"}},"outputId":"354d8a09-b50d-4ab6-938e-dee20e3f1a61"},"source":["z = torch.matmul(x, w)+b\n","print(z.requires_grad)\n","\n","with torch.no_grad():\n","    z = torch.matmul(x, w)+b\n","print(z.requires_grad)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["True\n","False\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hP0VaUxuYGRj"},"source":["同様に、``detach()`` をテンソルに使用することでも実現できます。\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"DQp79hmnYGRj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616113031997,"user_tz":-540,"elapsed":477,"user":{"displayName":"小川雄太郎","photoUrl":"","userId":"06190430902934159529"}},"outputId":"9f45c796-e753-4a5d-f8e9-2e6c5ea4d4df"},"source":["z = torch.matmul(x, w)+b\n","z_det = z.detach()\n","print(z_det.requires_grad)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["False\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oRcZx94AYGRj"},"source":["勾配の計算、追跡を不能にしたいケースはいく種類かあります。\n","\n","- ネットワークの一部のパラメータを固定したい（frozen parameters）ケース。これは[ファインチューニング](https://colab.research.google.com/github/YutaroOgawa/pytorch_tutorials_jp/blob/main/notebook/2_Image_Video/2_1_transfer_learning_tutorial_jp.ipynb)時によくあるケースです。\n","\n","\n","\n","- 順伝搬の計算スピードを高速化したいケース。\n"]},{"cell_type":"markdown","metadata":{"id":"uGqPd8IH2H9k"},"source":["---\n"]},{"cell_type":"markdown","metadata":{"id":"bnnX5dMjYGRj"},"source":["計算グラフについて補足\n","----------------------------\n","\n","概念的には、autogradはテンソルとそれらに対する演算処理を[`Function`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)を構成単位として、DAG（a directed acyclic graph）の形で保存したグラフです。\n","\n","\n","DAGにおいて、各leafは入力テンソル、そしてrootは出力テンソルです。\n","\n","このグラフをrootから各leafまでchain rule（微分の連鎖律）で追跡することによって各変数に対する偏微分の値を求めることができます。\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gXOdR8tpwcce"},"source":["順伝搬では autograd は2つの処理を同時に行っています。\n","\n","- 指定された演算を実行し、計算結果のテンソルを求める\n","- DAGの各操作の*gradient function* を更新する\n","\n","<br>\n","\n","逆伝搬では、``.backward()``がDAGのrootのテンソルに対して実行されると、autogradは、\n","\n","- 各変数の ``.grad_fn``を計算する\n","- 各変数の``.grad``属性に微分値を代入する\n","- 微分の連鎖律を使用して、各leafのテンソルの微分値を求める\n","\n","を行います。"]},{"cell_type":"markdown","metadata":{"id":"UsaoU-b1xVL8"},"source":["【注意】\n","\n","PyTorchではDAGは動的です（Functionで計算処理される際に逐次構築されていきます）。\n","\n","そして、`.backward()`を呼び出すたびに、autogradは再度新しいグラフを作成します。\n","\n","この特性こそが、モデルの順伝搬時に制御フロー文（if文やfor文）を使える理由であり、必要に応じて反復ごとに形や大きさ、操作を変えることができます。\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WMEbT6wZyAxp"},"source":["【日本語訳注】\n","\n","上記内容は初心者の方には非常に難しい話です。\n","\n","PyTorchは Define-by-run 形式であり、事前に計算グラフを定義するのではなく、計算を実行する際に、柔軟に計算グラフを作ってくれます。\n","\n","一方で、Define-and-run形式のディープラーニングフレームワーク（例えば、TensorFlow v1など）は、事前に計算グラフを定義する必要があるため、for文やif文といった制御フローの構文を柔軟に使いづらいです。\n","\n","このことを上記内容では説明しています。"]},{"cell_type":"markdown","metadata":{"id":"6fC7K9Up3w-A"},"source":["---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"P7LDZm9LynwY"},"source":["補注：テンソルに対する勾配とヤコビ行列\n","--------------------------------------\n","\n","多くの場合、スカラー値を出力する損失関数に対して、とある変数の勾配を計算します。\n","\n","ですが、関数の出力がスカラー値ではなく、任意のテンソルであるケースもあります。\n","\n","このような場合、PyTorchでは実際の勾配ではなく、いわゆるヤコビ行列（Jacobian\n","matrix）を計算することができます。"]},{"cell_type":"markdown","metadata":{"id":"-b3S418wzAav"},"source":["\n","ベクトル関数 $\\vec{y}=f(\\vec{x})$,において、\n","$\\vec{x}=\\langle x_1,\\dots,x_n\\rangle$ 、そして　$\\vec{y}=\\langle y_1,\\dots,y_m\\rangle$の場合、\n","\n","その勾配、 $\\vec{y}$ with respect to $\\vec{x}$ はヤコビ行列 で与えられます。"]},{"cell_type":"markdown","metadata":{"id":"-hUwX_Ei4qnM"},"source":["\\begin{split}\\begin{align}J=\\left(\\begin{array}{ccc}\n","   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n","   \\vdots & \\ddots & \\vdots\\\\\n","   \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n","   \\end{array}\\right)\\end{align}\\end{split}"]},{"cell_type":"markdown","metadata":{"id":"wAET0b7g0kJ_"},"source":["ヤコビ行列そのものを計算する代わりにPyTorchでは、**Jacobian Product**、 $v^T\\cdot J$ を、入力ベクトル$v=(v_1 \\dots v_m)$ に対して計算します。\n","\n","これは、$v$を引数として ``backward``メソッドを呼び出すことで計算されます。\n","\n","なお$v$ のサイズは積を計算したい、元のテンソルの大きさと同じである必要があります。\n"]},{"cell_type":"code","metadata":{"id":"JgQm2D6aYGRk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9c1d6195-631b-4523-af04-b7806542bb85"},"source":["inp = torch.eye(5, requires_grad=True)\n","out = (inp+1).pow(2)\n","out.backward(torch.ones_like(inp), retain_graph=True)\n","print(\"First call\\n\", inp.grad)\n","out.backward(torch.ones_like(inp), retain_graph=True)\n","print(\"\\nSecond call\\n\", inp.grad)\n","inp.grad.zero_()\n","out.backward(torch.ones_like(inp), retain_graph=True)\n","print(\"\\nCall after zeroing gradients\\n\", inp.grad)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["First call\n"," tensor([[4., 2., 2., 2., 2.],\n","        [2., 4., 2., 2., 2.],\n","        [2., 2., 4., 2., 2.],\n","        [2., 2., 2., 4., 2.],\n","        [2., 2., 2., 2., 4.]])\n","\n","Second call\n"," tensor([[8., 4., 4., 4., 4.],\n","        [4., 8., 4., 4., 4.],\n","        [4., 4., 8., 4., 4.],\n","        [4., 4., 4., 8., 4.],\n","        [4., 4., 4., 4., 8.]])\n","\n","Call after zeroing gradients\n"," tensor([[4., 2., 2., 2., 2.],\n","        [2., 4., 2., 2., 2.],\n","        [2., 2., 4., 2., 2.],\n","        [2., 2., 2., 4., 2.],\n","        [2., 2., 2., 2., 4.]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2oVqm6fqYGRl"},"source":["上記において、同じ変数`inp`に対して、``backward``を2回目実行した際には、勾配が異なる値になった点に注意してください。\n","\n","これはPyTorchでは``backward``を実行すると、勾配を蓄積（accumulate）する仕様だからです。\n","\n","すなわち、計算グラフの全leafの``grad``には、勾配が足し算されます。\n","\n","<br>\n","\n","そのため適切に勾配を計算するには、``grad``を事前に0にリセットする必要があります。\n","\n","なお実際にPyTorchでディープラーニングモデルの訓練を行う際には、**オプティマイザー（optimizer）**が、勾配をリセットする役割を担ってくれます。"]},{"cell_type":"markdown","metadata":{"id":"xysZvkN223T9"},"source":["【注意】\n","\n","本チュートリアルの最初の方で、``backward()``関数を引数のパラメータなしに実行していました。\n","\n","これは実質的には、``backward(torch.tensor(1.0))``を実行しているのと同じとなります。\n","\n","``backward()``はスカラー値の関数（例えば訓練時の損失関数）に対して、各パラメータの勾配を計算する際に便利です。"]},{"cell_type":"markdown","metadata":{"id":"8TFgRM-_YGRl"},"source":["--------------\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QWyQ2gZFYGRl"},"source":["さらなる詳細\n","--------------\n","以下のページも参考ください。\n","\n","- [`Autograd Mechanics`](https://pytorch.org/docs/stable/notes/autograd.html)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FoGLSieC3mwn"},"source":["以上。"]}]}